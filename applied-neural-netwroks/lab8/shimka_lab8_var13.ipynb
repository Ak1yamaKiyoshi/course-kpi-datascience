{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSqTcekVlnoh",
        "outputId": "5f295370-5f05-43da-9c07-6957147750d1"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLiQAffml3tB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "rsVed69yl54l",
        "outputId": "9639bbab-7169-4e72-e029-b0ec29cfeaf4"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
        "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\n",
        "wavs_path = data_path + \"/wavs/\"\n",
        "metadata_path = data_path + \"/metadata.csv\"\n",
        "\n",
        "\n",
        "# Read metadata file and parse it\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "metadata_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejywkEAAl7L9",
        "outputId": "5be59602-78c9-4772-f6f1-1041dfd75d50"
      },
      "outputs": [],
      "source": [
        "split = int(len(metadata_df) * 0.90)\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the training set: {len(df_val)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6-YjVqTl8WS",
        "outputId": "07427a50-e198-4d14-f568-071755377e79"
      },
      "outputs": [],
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G_NAAKfl-Nu"
      },
      "outputs": [],
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    label = tf.strings.lower(label)\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    label = char_to_num(label)\n",
        "    return spectrogram, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqTIHXW8l_0A"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "batch_size = 32\n",
        "# Define the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "8Jh8gg82mBfs",
        "outputId": "884953e8-29ba-4f3b-bf93-9bad7b3ffaa8"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "for batch in train_dataset.take(1):\n",
        "    spectrogram = batch[0][0].numpy()\n",
        "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "    label = batch[1][0]\n",
        "    # Spectrogram\n",
        "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    ax.imshow(spectrogram, vmax=1)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "    # Wav\n",
        "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = audio.numpy()\n",
        "    ax = plt.subplot(2, 1, 2)\n",
        "    plt.plot(audio)\n",
        "    ax.set_title(\"Signal Wave\")\n",
        "    ax.set_xlim(0, len(audio))\n",
        "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udg5bGxPmC9i"
      },
      "outputs": [],
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXKNin92mE8B",
        "outputId": "9dfe3059-157b-4740-862d-242bcf6cfb2c"
      },
      "outputs": [],
      "source": [
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model with BiLSTM layers.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.Bidirectional(\n",
        "            layers.LSTM(\n",
        "                units=rnn_units,\n",
        "                return_sequences=True,\n",
        "                name=f\"lstm_{i}\",\n",
        "            ),\n",
        "            name=f\"bidirectional_{i}\",\n",
        "        )\n",
        "        x = recurrent(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.3)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dnse_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.3)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"BiLSTM\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=512,\n",
        ")\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34jsTD3xmF9x"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpI6L4inmHvw",
        "outputId": "86ba5ddf-bffd-4f63-eb8a-ec6353cfb427"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 5\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwIUe9HJmJbx",
        "outputId": "2c0ed784-f870-4ae0-b77b-65044c4d5420"
      },
      "outputs": [],
      "source": [
        "# Let's check results on more validation samples\n",
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions = model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch_predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "print(\"-\" * 100)\n",
        "for i in np.random.randint(0, len(predictions), 5):\n",
        "    print(f\"Target    : {targets[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "ta0A2K9XmLJu",
        "outputId": "291f1e93-8cff-4111-d1c2-5005ee1ca94a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "print(history.history)\n",
        "# Plot loss history\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
